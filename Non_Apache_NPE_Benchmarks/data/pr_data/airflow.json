[
    {
        "pr": "https://github.com/apache/airflow/pull/15395",
        "repo": "airflow",
        "number": 15395,
        "bug_id": "airflow_15395",
        "title": "Bugfix: ``TypeError`` when Serializing & sorting iterables",
        "body": "This bug got introduced in #14909. Removed sorting from list and tuple as list & tuples preserve order unlike set.\r\n\r\nThe following DAG errors with: `TypeError: '<' not supported between instances of 'dict' and 'dict'`\r\n\r\n```python\r\nfrom airflow import models\r\nfrom airflow.operators.dummy import DummyOperator\r\nfrom datetime import datetime, timedelta\r\nparams = {\r\n    \"staging_schema\": [{\"key:\":\"foo\",\"value\":\"bar\"},\r\n                       {\"key:\":\"this\",\"value\":\"that\"}]\r\n}\r\n\r\nwith models.DAG(dag_id='test-dag',\r\n                start_date=datetime(2019, 2, 14),\r\n                schedule_interval='30 13 * * *',\r\n                catchup=False,\r\n                max_active_runs=1,\r\n                params=params\r\n                ) as dag:\r\n    my_task = DummyOperator(\r\n        task_id='task1'\r\n    )\r\n```\r\n\r\nFull Error:\r\n\r\n```\r\n  File \"/usr/local/lib/python3.7/site-packages/airflow/serialization/serialized_objects.py\", line 210, in <dictcomp>\r\n    return cls._encode({str(k): cls._serialize(v) for k, v in var.items()}, type_=DAT.DICT)\r\n  File \"/usr/local/lib/python3.7/site-packages/airflow/serialization/serialized_objects.py\", line 212, in _serialize\r\n    return sorted(cls._serialize(v) for v in var)\r\nTypeError: '<' not supported between instances of 'dict' and 'dict'\r\nDuring handling of the above exception, another exception occurred:\r\n...\r\n```\r\n\r\nThis is because `sorted()` does not work with dict as it can't compare. Removed sorting from list & tuples which fixes it.\r\nIt also fails when we have set with multiple types.\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n---\r\n**^ Add meaningful description above**\r\n\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).\r\n",
        "date": "2021-04-16",
        "merge_hash": "d1150403a35c497a774a4ffbb1ca4546c532dc81",
        "files": [
            {
                "sha": "8a6fdc89d206269149c50c21a2f2f5a45d9c534c",
                "filename": "airflow/serialization/serialized_objects.py",
                "status": "modified",
                "additions": 6,
                "deletions": 3,
                "changes": 9,
                "blob_url": "https://github.com/apache/airflow/blob/d1150403a35c497a774a4ffbb1ca4546c532dc81/airflow/serialization/serialized_objects.py",
                "raw_url": "https://github.com/apache/airflow/raw/d1150403a35c497a774a4ffbb1ca4546c532dc81/airflow/serialization/serialized_objects.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/serialization/serialized_objects.py?ref=d1150403a35c497a774a4ffbb1ca4546c532dc81"
            },
            {
                "sha": "6a186c5e57f3fbde29a53c68e9dbab509f6716d5",
                "filename": "tests/serialization/test_dag_serialization.py",
                "status": "modified",
                "additions": 29,
                "deletions": 7,
                "changes": 36,
                "blob_url": "https://github.com/apache/airflow/blob/d1150403a35c497a774a4ffbb1ca4546c532dc81/tests/serialization/test_dag_serialization.py",
                "raw_url": "https://github.com/apache/airflow/raw/d1150403a35c497a774a4ffbb1ca4546c532dc81/tests/serialization/test_dag_serialization.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/serialization/test_dag_serialization.py?ref=d1150403a35c497a774a4ffbb1ca4546c532dc81"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/14513",
        "repo": "airflow",
        "number": 14513,
        "bug_id": "airflow_14513",
        "title": "Bugfix: Fix TypeError in monitor_pod",
        "body": "We have seen several task failures because of this issue.\r\n\r\nIf the log read is interrupted before any logs are produced then\r\n`last_log_time` will not be set and the line\r\n`delta = pendulum.now() - last_log_time` will fail with\r\n```\r\nTypeError: unsupported operand type(s) for -: 'DateTime' and 'NoneType'\r\n```\r\n\r\nThis commit fix this issue by only updating `read_logs_since_sec` if\r\n`last_log_time` has been set.",
        "date": "2021-03-01",
        "merge_hash": "45a0ac2e01c174754f4e6612c8e4d3125061d096",
        "files": [
            {
                "sha": "3d663d287716fda60bf3894879d038b3d02f093f",
                "filename": "airflow/kubernetes/pod_launcher.py",
                "status": "modified",
                "additions": 4,
                "deletions": 3,
                "changes": 7,
                "blob_url": "https://github.com/apache/airflow/blob/45a0ac2e01c174754f4e6612c8e4d3125061d096/airflow/kubernetes/pod_launcher.py",
                "raw_url": "https://github.com/apache/airflow/raw/45a0ac2e01c174754f4e6612c8e4d3125061d096/airflow/kubernetes/pod_launcher.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/kubernetes/pod_launcher.py?ref=45a0ac2e01c174754f4e6612c8e4d3125061d096"
            },
            {
                "sha": "6e4026462b0a1dbd415dde9d4f84595a1ae2d778",
                "filename": "tests/kubernetes/test_pod_launcher.py",
                "status": "modified",
                "additions": 17,
                "deletions": 1,
                "changes": 18,
                "blob_url": "https://github.com/apache/airflow/blob/45a0ac2e01c174754f4e6612c8e4d3125061d096/tests/kubernetes/test_pod_launcher.py",
                "raw_url": "https://github.com/apache/airflow/raw/45a0ac2e01c174754f4e6612c8e4d3125061d096/tests/kubernetes/test_pod_launcher.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/kubernetes/test_pod_launcher.py?ref=45a0ac2e01c174754f4e6612c8e4d3125061d096"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/15132",
        "repo": "airflow",
        "number": 15132,
        "bug_id": "airflow_15132",
        "title": "Fix bug in airflow.stats timing that broke dogstatsd mode",
        "body": "The fix for this was very easy -- just a `timer` -> `timed` typo.\n\nHowever it turns out that the tests for airflow.stats were insufficient\nand didn't catch this, so I have extended the tests in two ways:\n\n1. Test all the other stat methods than just incr (guage, timer, timing,\n   decr)\n2. Use \"auto-specing\" feature of Mock to ensure that we can't make up\n   methods to call on a mock object.\n\n   > Autospeccing is based on the existing spec feature of mock.\n   > It limits the api of mocks to the api of an original object (the\n   > spec), but it is recursive (implemented lazily) so that attributes of\n   > mocks only have the same api as the attributes of the spec. In\n   > addition mocked functions / methods have the same call signature as\n   > the original so they raise a TypeError if they are called\n   > incorrectly.\n\nFixed #14839\n<!--\nThank you for contributing! Please make sure that your code changes\nare covered with tests. And in case of new features or big changes\nremember to adjust the documentation.\n\nFeel free to ping committers for the review!\n\nIn case of existing issue, reference it using one of the following:\n\ncloses: #ISSUE\nrelated: #ISSUE\n\nHow to write a good git commit message:\nhttp://chris.beams.io/posts/git-commit/\n-->\n\n---\n**^ Add meaningful description above**\n\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\nIn case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\nIn case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).",
        "date": "2021-04-01",
        "merge_hash": "b7cd2df056ac3ab113d77c5f6b65f02a77337907",
        "files": [
            {
                "sha": "34677daff2362d0f815e7850f06a0e8ec1ceeab3",
                "filename": "airflow/stats.py",
                "status": "modified",
                "additions": 1,
                "deletions": 1,
                "changes": 2,
                "blob_url": "https://github.com/apache/airflow/blob/b7cd2df056ac3ab113d77c5f6b65f02a77337907/airflow/stats.py",
                "raw_url": "https://github.com/apache/airflow/raw/b7cd2df056ac3ab113d77c5f6b65f02a77337907/airflow/stats.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/stats.py?ref=b7cd2df056ac3ab113d77c5f6b65f02a77337907"
            },
            {
                "sha": "83169e2935b2415a6f7099295a3f2e6140484485",
                "filename": "tests/core/test_stats.py",
                "status": "modified",
                "additions": 103,
                "deletions": 90,
                "changes": 193,
                "blob_url": "https://github.com/apache/airflow/blob/b7cd2df056ac3ab113d77c5f6b65f02a77337907/tests/core/test_stats.py",
                "raw_url": "https://github.com/apache/airflow/raw/b7cd2df056ac3ab113d77c5f6b65f02a77337907/tests/core/test_stats.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/core/test_stats.py?ref=b7cd2df056ac3ab113d77c5f6b65f02a77337907"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/14289",
        "repo": "airflow",
        "number": 14289,
        "bug_id": "airflow_14289",
        "title": "Don't allow SlackHook accept *args",
        "body": "Underlying Slack WebClient doesn't accept *args anymore\r\n\r\n<img width=\"482\" alt=\"Screenshot 2021-02-18 at 09 06 33\" src=\"https://user-images.githubusercontent.com/3982146/108320588-ad04ba00-71cb-11eb-9000-0fd946fd6f57.png\">\r\n\r\n```\r\n>>> from airflow.providers.slack.hooks.slack import SlackHook\r\n>>> alert = SlackHook('...')\r\n>>> alert.call('chat.postMessage', {'text': 'Igor is testing', 'channel': 'test-channel'})\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/khroliz/.local/share/virtualenvs/parcel-vGquKnoL/lib/python3.8/site-packages/airflow/providers/slack/hooks/slack.py\", line 103, in call\r\n    self.client.api_call(api_method, *args, **kwargs)\r\nTypeError: api_call() takes 2 positional arguments but 3 were given\r\n>>> alert.call('chat.postMessage', data={'text': 'Igor is testing', 'channel': 'test-channel'})\r\n```\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n---\r\n**^ Add meaningful description above**\r\n\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).\r\n",
        "date": "2021-02-18",
        "merge_hash": "8c060d55dfb3ded21cb9d2305cffe14e1c610680",
        "files": [
            {
                "sha": "1c55b8bf969e63b7a80b52a75f6111b49663e75c",
                "filename": "airflow/providers/slack/hooks/slack.py",
                "status": "modified",
                "additions": 2,
                "deletions": 2,
                "changes": 4,
                "blob_url": "https://github.com/apache/airflow/blob/8c060d55dfb3ded21cb9d2305cffe14e1c610680/airflow/providers/slack/hooks/slack.py",
                "raw_url": "https://github.com/apache/airflow/raw/8c060d55dfb3ded21cb9d2305cffe14e1c610680/airflow/providers/slack/hooks/slack.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/providers/slack/hooks/slack.py?ref=8c060d55dfb3ded21cb9d2305cffe14e1c610680"
            },
            {
                "sha": "0998aacfb162da83fb58553e211cb915fa552287",
                "filename": "tests/providers/slack/hooks/test_slack.py",
                "status": "modified",
                "additions": 1,
                "deletions": 1,
                "changes": 2,
                "blob_url": "https://github.com/apache/airflow/blob/8c060d55dfb3ded21cb9d2305cffe14e1c610680/tests/providers/slack/hooks/test_slack.py",
                "raw_url": "https://github.com/apache/airflow/raw/8c060d55dfb3ded21cb9d2305cffe14e1c610680/tests/providers/slack/hooks/test_slack.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/providers/slack/hooks/test_slack.py?ref=8c060d55dfb3ded21cb9d2305cffe14e1c610680"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/14686",
        "repo": "airflow",
        "number": 14686,
        "bug_id": "airflow_14686",
        "title": "Elasticsearch Provider: Fix logs downloading for tasks",
        "body": "Without this, Webserver fails with the following error. It happens because of https://github.com/apache/airflow/blob/99aab051600715a1ad029ce45a197a8492e5a151/airflow/providers/elasticsearch/log/es_task_handler.py#L165-L168:\r\n\r\n```\r\n[2021-03-09 18:55:19,640] {base.py:122} INFO - POST http://aa.aa:9200/_count [status:200 request:0.142s]\r\n[2021-03-09 18:55:19 +0000] [64] [ERROR] Error handling request\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/gunicorn/workers/sync.py\", line 181, in handle_request\r\n    for item in respiter:\r\n  File \"/usr/local/lib/python3.7/site-packages/werkzeug/wsgi.py\", line 506, in __next__\r\n    return self._next()\r\n  File \"/usr/local/lib/python3.7/site-packages/werkzeug/wrappers/base_response.py\", line 45, in _iter_encoded\r\n    for item in iterable:\r\n  File \"/usr/local/lib/python3.7/site-packages/airflow/utils/log/log_reader.py\", line 84, in read_log_stream\r\n    logs, metadata = self.read_log_chunks(ti, current_try_number, metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/airflow/utils/log/log_reader.py\", line 58, in read_log_chunks\r\n    logs, metadatas = self.log_handler.read(ti, try_number, metadata=metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py\", line 217, in read\r\n    log, metadata = self._read(task_instance, try_number_element, metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/airflow/providers/elasticsearch/log/es_task_handler.py\", line 186, in _read\r\n    and offset >= metadata['max_offset']\r\nTypeError: '>=' not supported between instances of 'str' and 'int'\r\n```\r\n\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n---\r\n**^ Add meaningful description above**\r\n\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).\r\n",
        "date": "2021-03-09",
        "merge_hash": "923bde2b917099135adfe470a5453f663131fd5f",
        "files": [
            {
                "sha": "2302d8b0a8d712263ecb204209a37516f6078d6d",
                "filename": "airflow/providers/elasticsearch/log/es_task_handler.py",
                "status": "modified",
                "additions": 2,
                "deletions": 2,
                "changes": 4,
                "blob_url": "https://github.com/apache/airflow/blob/923bde2b917099135adfe470a5453f663131fd5f/airflow/providers/elasticsearch/log/es_task_handler.py",
                "raw_url": "https://github.com/apache/airflow/raw/923bde2b917099135adfe470a5453f663131fd5f/airflow/providers/elasticsearch/log/es_task_handler.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/providers/elasticsearch/log/es_task_handler.py?ref=923bde2b917099135adfe470a5453f663131fd5f"
            },
            {
                "sha": "1262c26161be5c22e9ef8491b1037d89b7ef12fe",
                "filename": "tests/providers/elasticsearch/log/test_es_task_handler.py",
                "status": "modified",
                "additions": 1,
                "deletions": 1,
                "changes": 2,
                "blob_url": "https://github.com/apache/airflow/blob/923bde2b917099135adfe470a5453f663131fd5f/tests/providers/elasticsearch/log/test_es_task_handler.py",
                "raw_url": "https://github.com/apache/airflow/raw/923bde2b917099135adfe470a5453f663131fd5f/tests/providers/elasticsearch/log/test_es_task_handler.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/providers/elasticsearch/log/test_es_task_handler.py?ref=923bde2b917099135adfe470a5453f663131fd5f"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/14436",
        "repo": "airflow",
        "number": 14436,
        "bug_id": "airflow_14436",
        "title": "BugFix: Serialize max_retry_delay as a timedelta",
        "body": "closes: #13086, #14212\r\n\r\nWhilst testing upgrading from 1.10.12 to 2.0.1 I've been seeing the following scheduler error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py\", line 1280, in _execute\r\n    self._run_scheduler_loop()\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py\", line 1382, in _run_scheduler_loop\r\n    num_queued_tis = self._do_scheduling(session)\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py\", line 1521, in _do_scheduling\r\n    self._schedule_dag_run(dag_run, active_runs_by_dag_id.get(dag_run.dag_id, set()), session)\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py\", line 1760, in _schedule_dag_run\r\n    schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/utils/session.py\", line 62, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/models/dagrun.py\", line 406, in update_state\r\n    info = self.task_instance_scheduling_decisions(session)\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/utils/session.py\", line 62, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/models/dagrun.py\", line 507, in task_instance_scheduling_decisions\r\n    schedulable_tis, changed_tis = self._get_ready_tis(scheduleable_tasks, finished_tasks, session)\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/models/dagrun.py\", line 535, in _get_ready_tis\r\n    session=session,\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/utils/session.py\", line 62, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/models/taskinstance.py\", line 838, in are_dependencies_met\r\n    for dep_status in self.get_failed_dep_statuses(dep_context=dep_context, session=session):\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/models/taskinstance.py\", line 859, in get_failed_dep_statuses\r\n    for dep_status in dep.get_dep_statuses(self, session, dep_context):\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/ti_deps/deps/base_ti_dep.py\", line 101, in get_dep_statuses\r\n    yield from self._get_dep_statuses(ti, session, dep_context)\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/ti_deps/deps/not_in_retry_period_dep.py\", line 47, in _get_dep_statuses\r\n    next_task_retry_date = ti.next_retry_datetime()\r\n  File \"/app/.heroku/python/lib/python3.7/site-packages/airflow/models/taskinstance.py\", line 905, in next_retry_datetime\r\n    delay = min(self.task.max_retry_delay, delay)\r\nTypeError: '<' not supported between instances of 'datetime.timedelta' and 'float'\r\n```\r\n\r\nIt appears that the `max_retry_delay` value is not being serialised as a timedelta type. This PR seeks to address that.\r\n",
        "date": "2021-02-26",
        "merge_hash": "59c459fa2a6aafc133db4a89980fb3d3d0d25589",
        "files": [
            {
                "sha": "06094a1c68f62dbf58ff0b492e8d287505bbbdc1",
                "filename": "airflow/models/baseoperator.py",
                "status": "modified",
                "additions": 8,
                "deletions": 1,
                "changes": 9,
                "blob_url": "https://github.com/apache/airflow/blob/59c459fa2a6aafc133db4a89980fb3d3d0d25589/airflow/models/baseoperator.py",
                "raw_url": "https://github.com/apache/airflow/raw/59c459fa2a6aafc133db4a89980fb3d3d0d25589/airflow/models/baseoperator.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/models/baseoperator.py?ref=59c459fa2a6aafc133db4a89980fb3d3d0d25589"
            },
            {
                "sha": "0fbe20fb23eff59aeb79f1dc32aaa7a1008fe663",
                "filename": "airflow/serialization/schema.json",
                "status": "modified",
                "additions": 1,
                "deletions": 0,
                "changes": 1,
                "blob_url": "https://github.com/apache/airflow/blob/59c459fa2a6aafc133db4a89980fb3d3d0d25589/airflow/serialization/schema.json",
                "raw_url": "https://github.com/apache/airflow/raw/59c459fa2a6aafc133db4a89980fb3d3d0d25589/airflow/serialization/schema.json",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/serialization/schema.json?ref=59c459fa2a6aafc133db4a89980fb3d3d0d25589"
            },
            {
                "sha": "d609c0986d09bed8d87174a32f767195800f2781",
                "filename": "airflow/serialization/serialized_objects.py",
                "status": "modified",
                "additions": 1,
                "deletions": 1,
                "changes": 2,
                "blob_url": "https://github.com/apache/airflow/blob/59c459fa2a6aafc133db4a89980fb3d3d0d25589/airflow/serialization/serialized_objects.py",
                "raw_url": "https://github.com/apache/airflow/raw/59c459fa2a6aafc133db4a89980fb3d3d0d25589/airflow/serialization/serialized_objects.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/serialization/serialized_objects.py?ref=59c459fa2a6aafc133db4a89980fb3d3d0d25589"
            },
            {
                "sha": "c445e194cfe0973a185736d64d38a6c77ff440d9",
                "filename": "tests/serialization/test_dag_serialization.py",
                "status": "modified",
                "additions": 4,
                "deletions": 0,
                "changes": 4,
                "blob_url": "https://github.com/apache/airflow/blob/59c459fa2a6aafc133db4a89980fb3d3d0d25589/tests/serialization/test_dag_serialization.py",
                "raw_url": "https://github.com/apache/airflow/raw/59c459fa2a6aafc133db4a89980fb3d3d0d25589/tests/serialization/test_dag_serialization.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/serialization/test_dag_serialization.py?ref=59c459fa2a6aafc133db4a89980fb3d3d0d25589"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/12675",
        "repo": "airflow",
        "number": 12675,
        "bug_id": "airflow_12675",
        "title": "Ensure that tasks set to up_for_retry have an end date",
        "body": "If a task is \"manually\" set to up_for_retry (via the UI for instance) it\r\nmight not have an end date, and much of the logic about computing\r\nretries assumes that it does.\r\n    \r\nWithout this, manually setting a running task to up_for_retry will make\r\nthe make it impossible to view the TaskInstance details page (as it\r\ntries to print the is_premature property), and also the NotInRetryPeriod\r\nTIDep fails - both with an exception:\r\n    \r\n> File \"airflow/models/taskinstance.py\", line 882, in next_retry_datetime\r\n>   return self.end_date + delay\r\n> TypeError: unsupported operand type(s) for +: 'NoneType' and 'datetime.timedelta'\r\n",
        "date": "2020-11-29",
        "merge_hash": "8291fabaf95dd4dc2ff2ab33be07ccd1a34df2ae",
        "files": [
            {
                "sha": "c445df843a9eab9277a00dc2afefc7ca37c9d056",
                "filename": "airflow/models/taskinstance.py",
                "status": "modified",
                "additions": 4,
                "deletions": 4,
                "changes": 8,
                "blob_url": "https://github.com/apache/airflow/blob/8291fabaf95dd4dc2ff2ab33be07ccd1a34df2ae/airflow/models/taskinstance.py",
                "raw_url": "https://github.com/apache/airflow/raw/8291fabaf95dd4dc2ff2ab33be07ccd1a34df2ae/airflow/models/taskinstance.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/models/taskinstance.py?ref=8291fabaf95dd4dc2ff2ab33be07ccd1a34df2ae"
            },
            {
                "sha": "d4c5468d1a0cdf0eee9b55cbe50260277f30ecb2",
                "filename": "tests/models/test_taskinstance.py",
                "status": "modified",
                "additions": 14,
                "deletions": 0,
                "changes": 14,
                "blob_url": "https://github.com/apache/airflow/blob/8291fabaf95dd4dc2ff2ab33be07ccd1a34df2ae/tests/models/test_taskinstance.py",
                "raw_url": "https://github.com/apache/airflow/raw/8291fabaf95dd4dc2ff2ab33be07ccd1a34df2ae/tests/models/test_taskinstance.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/models/test_taskinstance.py?ref=8291fabaf95dd4dc2ff2ab33be07ccd1a34df2ae"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/12094",
        "repo": "airflow",
        "number": 12094,
        "bug_id": "airflow_12094",
        "title": "Correct reading grpc credentials file",
        "body": "gRPC is expecting the credentials to be bytes rather than a string.\r\nTypeError: expected certificate to be bytes, got <class 'str'>\r\n\r\nThis is the fix suggested in the grpc repo.\r\nhttps://github.com/grpc/grpc/issues/16718#issuecomment-436046373\r\n\r\nWithout this fix, you get the following error:\r\n```\r\n  File \"/home/airflow/.local/lib/python3.6/site-packages/airflow/contrib/hooks/grpc_hook.py\", line 102, in run\r\n    with self.get_conn() as channel:\r\n  File \"/home/airflow/.local/lib/python3.6/site-packages/airflow/contrib/hooks/grpc_hook.py\", line 68, in get_conn\r\n    creds = grpc.ssl_channel_credentials(open(credential_file_name).read())\r\n  File \"/home/airflow/.local/lib/python3.6/site-packages/grpc/__init__.py\", line 1607, in ssl_channel_credentials\r\n    certificate_chain))\r\n  File \"src/python/grpcio/grpc/_cython/_cygrpc/credentials.pyx.pxi\", line 134, in grpc._cython.cygrpc.SSLChannelCredentials.__cinit__\r\nTypeError: expected certificate to be bytes, got <class 'str'>\r\n```\r\n<!--\r\nThank you for contributing! Please make sure that your code changes\r\nare covered with tests. And in case of new features or big changes\r\nremember to adjust the documentation.\r\n\r\nFeel free to ping committers for the review!\r\n\r\nIn case of existing issue, reference it using one of the following:\r\n\r\ncloses: #ISSUE\r\nrelated: #ISSUE\r\n\r\nHow to write a good git commit message:\r\nhttp://chris.beams.io/posts/git-commit/\r\n-->\r\n\r\n---\r\n**^ Add meaningful description above**\r\n\r\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\r\nIn case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).\r\n",
        "date": "2020-11-04",
        "merge_hash": "fcfc7f12421bd35a366324fe7814c90da8de5735",
        "files": [
            {
                "sha": "ccfbd8f166a50fad73a385a4dbc4311057f35b11",
                "filename": "airflow/providers/grpc/hooks/grpc.py",
                "status": "modified",
                "additions": 2,
                "deletions": 1,
                "changes": 3,
                "blob_url": "https://github.com/apache/airflow/blob/fcfc7f12421bd35a366324fe7814c90da8de5735/airflow/providers/grpc/hooks/grpc.py",
                "raw_url": "https://github.com/apache/airflow/raw/fcfc7f12421bd35a366324fe7814c90da8de5735/airflow/providers/grpc/hooks/grpc.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/providers/grpc/hooks/grpc.py?ref=fcfc7f12421bd35a366324fe7814c90da8de5735"
            },
            {
                "sha": "e42002222669d2fdf46ed66f5b4eb7d5df220e2f",
                "filename": "tests/providers/grpc/hooks/test_grpc.py",
                "status": "modified",
                "additions": 2,
                "deletions": 2,
                "changes": 4,
                "blob_url": "https://github.com/apache/airflow/blob/fcfc7f12421bd35a366324fe7814c90da8de5735/tests/providers/grpc/hooks/test_grpc.py",
                "raw_url": "https://github.com/apache/airflow/raw/fcfc7f12421bd35a366324fe7814c90da8de5735/tests/providers/grpc/hooks/test_grpc.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/providers/grpc/hooks/test_grpc.py?ref=fcfc7f12421bd35a366324fe7814c90da8de5735"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/8151",
        "repo": "airflow",
        "number": 8151,
        "bug_id": "airflow_8151",
        "title": "BugFix: Fix writing & deleting Dag Code for Serialized DAGs",
        "body": "With `store_dag_code` enabled scheduler throws the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/multiprocessing/process.py\", line\r\n297, in _bootstrap\r\n    self.run()\r\n  File \"/usr/local/lib/python3.7/multiprocessing/process.py\", line 99, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py\",\r\nline 158, in _run_file_processor\r\n    pickle_dags)\r\n  File \"/usr/local/lib/python3.7/site-packages/airflow/utils/db.py\",\r\nline 74, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py\",\r\nline 1582, in process_file\r\n    dag.sync_to_db()\r\n  File \"/usr/local/lib/python3.7/site-packages/airflow/utils/db.py\",\r\nline 74, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/airflow/models/dag.py\",\r\nline 1519, in sync_to_db\r\n    DagCode.bulk_sync_to_db([dag.fileloc for dag in orm_dag])\r\nTypeError: 'DagModel' object is not iterable\r\n```\r\n\r\nThis is a blocker for 1.10.10 and the bug was found in 1.10.10rc2 by Kostya Esmukov \r\n\r\n---\r\nMake sure to mark the boxes below before creating PR: [x]\r\n\r\n- [x] Description above provides context of the change\r\n- [x] Unit tests coverage for changes (not needed for documentation changes)\r\n- [x] Commits follow \"[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)\"\r\n- [x] Relevant documentation is updated including usage instructions.\r\n- [x] I will engage committers as explained in [Contribution Workflow Example](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#contribution-workflow-example).\r\n\r\n---\r\nIn case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).\r\nRead the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.\r\n",
        "date": "2020-04-05",
        "merge_hash": "892d4dbbbaed9f97c8cc937bdf6a87726dd2d2ad",
        "files": [
            {
                "sha": "b2758bf45651dbccabf4ebea6ff9872fe48f980a",
                "filename": "airflow/models/dag.py",
                "status": "modified",
                "additions": 1,
                "deletions": 1,
                "changes": 2,
                "blob_url": "https://github.com/apache/airflow/blob/892d4dbbbaed9f97c8cc937bdf6a87726dd2d2ad/airflow/models/dag.py",
                "raw_url": "https://github.com/apache/airflow/raw/892d4dbbbaed9f97c8cc937bdf6a87726dd2d2ad/airflow/models/dag.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/models/dag.py?ref=892d4dbbbaed9f97c8cc937bdf6a87726dd2d2ad"
            },
            {
                "sha": "e96e7e450ba17096dd922ba7ac7ee2ef96d4605d",
                "filename": "airflow/models/dagcode.py",
                "status": "modified",
                "additions": 5,
                "deletions": 5,
                "changes": 10,
                "blob_url": "https://github.com/apache/airflow/blob/892d4dbbbaed9f97c8cc937bdf6a87726dd2d2ad/airflow/models/dagcode.py",
                "raw_url": "https://github.com/apache/airflow/raw/892d4dbbbaed9f97c8cc937bdf6a87726dd2d2ad/airflow/models/dagcode.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/models/dagcode.py?ref=892d4dbbbaed9f97c8cc937bdf6a87726dd2d2ad"
            },
            {
                "sha": "24d518adb317b039106ab7f63d4bf0fc694b5aa7",
                "filename": "tests/models/test_dagcode.py",
                "status": "modified",
                "additions": 5,
                "deletions": 1,
                "changes": 6,
                "blob_url": "https://github.com/apache/airflow/blob/892d4dbbbaed9f97c8cc937bdf6a87726dd2d2ad/tests/models/test_dagcode.py",
                "raw_url": "https://github.com/apache/airflow/raw/892d4dbbbaed9f97c8cc937bdf6a87726dd2d2ad/tests/models/test_dagcode.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/models/test_dagcode.py?ref=892d4dbbbaed9f97c8cc937bdf6a87726dd2d2ad"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/7092",
        "repo": "airflow",
        "number": 7092,
        "bug_id": "airflow_7092",
        "title": "[AIRFLOW-6347] BugFix: Can't get task logs when serialization is enabled",
        "body": "Quoting from Jira:\r\n\r\n**Problem**:\r\nWhen I set next config options in `airflow.cfg`:\r\n\r\n```ini\r\n[core]\r\nstore_serialized_dags = True\r\nmin_serialized_dag_update_interval = 3\r\n ```\r\n\r\nView with task logs shows infinity Js spinner, while in webserver log I see next error:\r\n```\r\n[2019-12-25 14:48:57,640] {{app.py:1891}} ERROR - Exception on /get_logs_with_metadata [GET]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 2446, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 1951, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 1820, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"/usr/local/lib/python3.7/site-packages/flask/_compat.py\", line 39, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 1949, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 1935, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"/usr/local/airflow/.local/lib/python3.7/site-packages/airflow/www_rbac/decorators.py\", line 121, in wrapper\r\n    return f(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/flask_appbuilder/security/decorators.py\", line 101, in wraps\r\n    return f(self, *args, **kwargs)\r\n  File \"/usr/local/airflow/.local/lib/python3.7/site-packages/airflow/www_rbac/decorators.py\", line 56, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/usr/local/airflow/.local/lib/python3.7/site-packages/airflow/utils/db.py\", line 74, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/airflow/.local/lib/python3.7/site-packages/airflow/www_rbac/views.py\", line 637, in get_logs_with_metadata\r\n    logs, metadata = _get_logs_with_metadata(try_number, metadata)\r\n  File \"/usr/local/airflow/.local/lib/python3.7/site-packages/airflow/www_rbac/views.py\", line 628, in _get_logs_with_metadata\r\n    logs, metadatas = handler.read(ti, try_number, metadata=metadata)\r\n  File \"/usr/local/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py\", line 169, in read\r\n    log, metadata = self._read(task_instance, try_number_element, metadata)\r\n  File \"/usr/local/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py\", line 98, in _read\r\n    log_relative_path = self._render_filename(ti, try_number)\r\n  File \"/usr/local/airflow/.local/lib/python3.7/site-packages/airflow/utils/log/file_task_handler.py\", line 75, in _render_filename\r\n    jinja_context = ti.get_template_context()\r\n  File \"/usr/local/airflow/.local/lib/python3.7/site-packages/airflow/utils/db.py\", line 74, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py\", line 1149, in get_template_context\r\n    if 'tables' in task.params:\r\nTypeError: argument of type 'NoneType' is not iterable\r\n```\r\n\r\n**Solution:**\r\nThis is because we use `self.params = params or {}` and where default value of `params` is `None` in DAG and Operators and Dag S10n ignore all the None field. \r\n\r\nWhile de-serializing `params` was being set to None because of the following line:\r\n\r\n```python\r\nkeys_to_set_none = dag.get_serialized_fields() - encoded_dag.keys() - cls._CONSTRUCTOR_PARAMS.keys()\r\n        for k in keys_to_set_none:\r\n            setattr(dag, k, None)\r\n```\r\n\r\n\r\n---\r\nIssue link: [AIRFLOW-6347](https://issues.apache.org/jira/browse/AIRFLOW-6347/)\r\n\r\n- [x] Description above provides context of the change\r\n- [x] Commit message/PR title starts with `[AIRFLOW-NNNN]`. AIRFLOW-NNNN = JIRA ID<sup>*</sup>\r\n- [x] Unit tests coverage for changes (not needed for documentation changes)\r\n- [x] Commits follow \"[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)\"\r\n- [x] Relevant documentation is updated including usage instructions.\r\n- [x] I will engage committers as explained in [Contribution Workflow Example](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#contribution-workflow-example).\r\n\r\n<sup>*</sup> For document-only changes commit message can start with `[AIRFLOW-XXXX]`.\r\n\r\n---\r\nIn case of fundamental code change, Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)) is needed.\r\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\nIn case of backwards incompatible changes please leave a note in [UPDATING.md](https://github.com/apache/airflow/blob/master/UPDATING.md).\r\nRead the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.\r\n",
        "date": "2020-01-07",
        "merge_hash": "96ee8bde80d49bb31391bb3b73ce776855b73670",
        "files": [
            {
                "sha": "d20dd3ddabdd644ff3bef192f28e25c165040a6b",
                "filename": "airflow/serialization/schema.json",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/apache/airflow/blob/96ee8bde80d49bb31391bb3b73ce776855b73670/airflow/serialization/schema.json",
                "raw_url": "https://github.com/apache/airflow/raw/96ee8bde80d49bb31391bb3b73ce776855b73670/airflow/serialization/schema.json",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/serialization/schema.json?ref=96ee8bde80d49bb31391bb3b73ce776855b73670"
            },
            {
                "sha": "d39bd5060836236aa4af6b57cf6eabd3193d821e",
                "filename": "airflow/serialization/serialized_objects.py",
                "status": "modified",
                "additions": 9,
                "deletions": 7,
                "changes": 16,
                "blob_url": "https://github.com/apache/airflow/blob/96ee8bde80d49bb31391bb3b73ce776855b73670/airflow/serialization/serialized_objects.py",
                "raw_url": "https://github.com/apache/airflow/raw/96ee8bde80d49bb31391bb3b73ce776855b73670/airflow/serialization/serialized_objects.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/serialization/serialized_objects.py?ref=96ee8bde80d49bb31391bb3b73ce776855b73670"
            },
            {
                "sha": "ca91f02bc45f5dd720a51d3f7a65cdf0f5e8b991",
                "filename": "airflow/utils/decorators.py",
                "status": "modified",
                "additions": 1,
                "deletions": 1,
                "changes": 2,
                "blob_url": "https://github.com/apache/airflow/blob/96ee8bde80d49bb31391bb3b73ce776855b73670/airflow/utils/decorators.py",
                "raw_url": "https://github.com/apache/airflow/raw/96ee8bde80d49bb31391bb3b73ce776855b73670/airflow/utils/decorators.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/utils/decorators.py?ref=96ee8bde80d49bb31391bb3b73ce776855b73670"
            },
            {
                "sha": "ae2cbea8894d13022eafa7acb8120412698d4c54",
                "filename": "tests/serialization/test_dag_serialization.py",
                "status": "modified",
                "additions": 68,
                "deletions": 2,
                "changes": 70,
                "blob_url": "https://github.com/apache/airflow/blob/96ee8bde80d49bb31391bb3b73ce776855b73670/tests/serialization/test_dag_serialization.py",
                "raw_url": "https://github.com/apache/airflow/raw/96ee8bde80d49bb31391bb3b73ce776855b73670/tests/serialization/test_dag_serialization.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/serialization/test_dag_serialization.py?ref=96ee8bde80d49bb31391bb3b73ce776855b73670"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/6036",
        "repo": "airflow",
        "number": 6036,
        "bug_id": "airflow_6036",
        "title": "[AIRFLOW-5428] Dataflow with one job is not done correctly",
        "body": "Details information is available here: https://github.com/apache/airflow/pull/4633#discussion_r321568473\r\n```\r\n[2019-09-06 03:20:51,974] {taskinstance.py:1042} ERROR - string indices must be integers\r\nTraceback (most recent call last):\r\n  File \"/opt/airflow/airflow/models/taskinstance.py\", line 917, in _run_raw_task\r\n    result = task_copy.execute(context=context)\r\n  File \"/opt/airflow/airflow/gcp/operators/dataflow.py\", line 216, in execute\r\n    self.jar, self.job_class, True, self.multiple_jobs)\r\n  File \"/opt/airflow/airflow/gcp/hooks/dataflow.py\", line 372, in start_java_dataflow\r\n    self._start_dataflow(variables, name, command_prefix, label_formatter, multiple_jobs)\r\n  File \"/opt/airflow/airflow/contrib/hooks/gcp_api_base_hook.py\", line 307, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"/opt/airflow/airflow/gcp/hooks/dataflow.py\", line 327, in _start_dataflow\r\n    variables['region'], self.poll_sleep, job_id, self.num_retries, multiple_jobs) \\\r\n  File \"/opt/airflow/airflow/gcp/hooks/dataflow.py\", line 76, in __init__\r\n    self._jobs = self._get_jobs()\r\n  File \"/opt/airflow/airflow/gcp/hooks/dataflow.py\", line 138, in _get_jobs\r\n    self._job_id, job['name']\r\nTypeError: string indices must be integers\r\n```\r\nCC: @Fokko  You did a review of the changes that introduced these regressions. Could you check if I missed something?\r\n\r\nI would like to test it  using system tests. Unfortunately, they are not finished yet, but I work hard on it - https://github.com/apache/airflow/pull/6035. When I confirm that the code works in all case, this change can be merge.\r\n\r\n---\r\n\r\nMake sure you have checked _all_ steps below.\r\n\r\n### Jira\r\n\r\n- [ ] My PR addresses the following [Airflow Jira](https://issues.apache.org/jira/browse/AIRFLOW/) issues and references them in the PR title. For example, \"\\[AIRFLOW-XXX\\] My Airflow PR\"\r\n  - https://issues.apache.org/jira/browse/AIRFLOW-5428\r\n  - In case you are fixing a typo in the documentation you can prepend your commit with \\[AIRFLOW-XXX\\], code changes always need a Jira issue.\r\n  - In case you are proposing a fundamental code change, you need to create an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)).\r\n  - In case you are adding a dependency, check if the license complies with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\n\r\n### Description\r\n\r\n- [ ] Here are some details about my PR, including screenshots of any UI changes:\r\n\r\n### Tests\r\n\r\n- [ ] My PR adds the following unit tests __OR__ does not need testing for this extremely good reason:\r\n\r\n### Commits\r\n\r\n- [ ] My commits all reference Jira issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from \"[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)\":\r\n  1. Subject is separated from body by a blank line\r\n  1. Subject is limited to 50 characters (not including Jira issue reference)\r\n  1. Subject does not end with a period\r\n  1. Subject uses the imperative mood (\"add\", not \"adding\")\r\n  1. Body wraps at 72 characters\r\n  1. Body explains \"what\" and \"why\", not \"how\"\r\n\r\n### Documentation\r\n\r\n- [ ] In case of new functionality, my PR adds documentation that describes how to use it.\r\n  - All the public functions and the classes in the PR contain docstrings that explain what it does\r\n  - If you implement backwards incompatible changes, please leave a note in the [Updating.md](https://github.com/apache/airflow/blob/master/UPDATING.md) so we can assign it to a appropriate release\r\n",
        "date": "2019-09-15",
        "merge_hash": "52d9e6a64b0308af2afb4cac3762a93e53108588",
        "files": [
            {
                "sha": "d8bce66f3ab2d78374273bb9d3b2984d20dad857",
                "filename": "airflow/gcp/hooks/dataflow.py",
                "status": "modified",
                "additions": 8,
                "deletions": 5,
                "changes": 13,
                "blob_url": "https://github.com/apache/airflow/blob/52d9e6a64b0308af2afb4cac3762a93e53108588/airflow/gcp/hooks/dataflow.py",
                "raw_url": "https://github.com/apache/airflow/raw/52d9e6a64b0308af2afb4cac3762a93e53108588/airflow/gcp/hooks/dataflow.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/gcp/hooks/dataflow.py?ref=52d9e6a64b0308af2afb4cac3762a93e53108588"
            },
            {
                "sha": "5a7b7dce203f8e07fe1c8eae82e709600438dfd4",
                "filename": "tests/gcp/hooks/test_dataflow.py",
                "status": "modified",
                "additions": 64,
                "deletions": 2,
                "changes": 66,
                "blob_url": "https://github.com/apache/airflow/blob/52d9e6a64b0308af2afb4cac3762a93e53108588/tests/gcp/hooks/test_dataflow.py",
                "raw_url": "https://github.com/apache/airflow/raw/52d9e6a64b0308af2afb4cac3762a93e53108588/tests/gcp/hooks/test_dataflow.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/gcp/hooks/test_dataflow.py?ref=52d9e6a64b0308af2afb4cac3762a93e53108588"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/5686",
        "repo": "airflow",
        "number": 5686,
        "bug_id": "airflow_5686",
        "title": "[AIRFLOW-5075] Let HttpHook handle connections with empty host fields",
        "body": "Make sure you have checked _all_ steps below.\r\n\r\n### Jira\r\n\r\n- [X] My PR addresses the following [Airflow Jira](https://issues.apache.org/jira/browse/AIRFLOW/) issues and references them in the PR title. For example, \"\\[AIRFLOW-XXX\\] My Airflow PR\"\r\n  - https://issues.apache.org/jira/browse/AIRFLOW-XXX\r\n  - In case you are fixing a typo in the documentation you can prepend your commit with \\[AIRFLOW-XXX\\], code changes always need a Jira issue.\r\n  - In case you are proposing a fundamental code change, you need to create an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvements+Proposals)).\r\n  - In case you are adding a dependency, check if the license complies with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\r\n\r\n### Description\r\n\r\n- [X] Here are some details about my PR, including screenshots of any UI changes:\r\n\r\nLet HttpHook handle connections with empty host fields, which would throw an exception previously:\r\n\r\n```\r\n  File \"/home/ubuntu/.local/lib/python3.6/site-packages/airflow/hooks/http_hook.py\", line 63, in get_conn\r\n    if \"://\" in conn.host:\r\nTypeError: argument of type 'NoneType' is not iterable\r\n```\r\n\r\n### Tests\r\n\r\n- [X] My PR adds the following unit tests __OR__ does not need testing for this extremely good reason:\r\n\r\n- `test_connection_without_host`\r\n\r\n### Commits\r\n\r\n- [X] My commits all reference Jira issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from \"[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)\":\r\n  1. Subject is separated from body by a blank line\r\n  1. Subject is limited to 50 characters (not including Jira issue reference)\r\n  1. Subject does not end with a period\r\n  1. Subject uses the imperative mood (\"add\", not \"adding\")\r\n  1. Body wraps at 72 characters\r\n  1. Body explains \"what\" and \"why\", not \"how\"\r\n\r\n### Documentation\r\n\r\n- [X] In case of new functionality, my PR adds documentation that describes how to use it.\r\n  - All the public functions and the classes in the PR contain docstrings that explain what it does\r\n  - If you implement backwards incompatible changes, please leave a note in the [Updating.md](https://github.com/apache/airflow/blob/master/UPDATING.md) so we can assign it to a appropriate release\r\n\r\nNo docs; trivial change\r\n\r\n### Code Quality\r\nLet HttpHook handle connections with empty host fields\r\n- [X] Passes `flake8`\r\n",
        "date": "2019-07-31",
        "merge_hash": "63765487bb75e056ca12700d6c97c1e622d378cc",
        "files": [
            {
                "sha": "a7c8d99ddefe1213f2059415b9c9337b408cf02d",
                "filename": "airflow/hooks/http_hook.py",
                "status": "modified",
                "additions": 3,
                "deletions": 2,
                "changes": 5,
                "blob_url": "https://github.com/apache/airflow/blob/63765487bb75e056ca12700d6c97c1e622d378cc/airflow/hooks/http_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/63765487bb75e056ca12700d6c97c1e622d378cc/airflow/hooks/http_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/hooks/http_hook.py?ref=63765487bb75e056ca12700d6c97c1e622d378cc"
            },
            {
                "sha": "e9722d299de0e0246af6cb400cb3b4159b457237",
                "filename": "tests/hooks/test_http_hook.py",
                "status": "modified",
                "additions": 9,
                "deletions": 0,
                "changes": 9,
                "blob_url": "https://github.com/apache/airflow/blob/63765487bb75e056ca12700d6c97c1e622d378cc/tests/hooks/test_http_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/63765487bb75e056ca12700d6c97c1e622d378cc/tests/hooks/test_http_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/hooks/test_http_hook.py?ref=63765487bb75e056ca12700d6c97c1e622d378cc"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/5955",
        "repo": "airflow",
        "number": 5955,
        "bug_id": "airflow_5955",
        "title": "[AIRFLOW-5350] Fix bug in the num_retires field in BigQueryHook",
        "body": "Make sure you have checked _all_ steps below.\r\n\r\n### Jira\r\n\r\n- [x] My PR addresses the following [Airflow Jira](https://issues.apache.org/jira/browse/AIRFLOW/) issues and references them in the PR title. For example, \"\\[AIRFLOW-XXX\\] My Airflow PR\"\r\n  - https://issues.apache.org/jira/browse/AIRFLOW-5350\r\n\r\n### Description\r\n\r\n- [x] Here are some details about my PR, including screenshots of any UI changes:\r\nThe `num_retries` extra is no set in old connections that were created before 1.10.4, for those fields it's value is None which causes the below error:\r\n\r\nFrom the StackOverflow Post:\r\n\r\n```\r\n [2019-08-27 02:49:58,076] {cli.py:516} INFO - Running <TaskInstance: cadastro_remessas_paises2.gcs_to_bq 2019-08-27T02:42:43.970619+00:00 [running]> on host cadastroremessaspaises2gcstobq-78f1ea099c3b4e718ba707cb03ffda1e \r\n    [2019-08-27 02:49:58,136] {logging_mixin.py:95} INFO - [[34m2019-08-27 02:49:58,136[0m] {[34mdiscovery.py:[0m271} INFO[0m - URL being requested: GET [1mhttps://www.googleapis.com/discovery/v1/apis/bigquery/v2/rest[0m[0m \r\n    [2019-08-27 02:49:59,259] {logging_mixin.py:95} INFO - [[34m2019-08-27 02:49:59,259[0m] {[34mmy_functions_google.py:[0m2224} INFO[0m - Project not included in [1mdestination_project_dataset_table[0m: [1mcadastro_remessas.paises2[0m; using project \"[1mbigdata-staging[0m\"[0m \r\n    [2019-08-27 02:49:59,266] {logging_mixin.py:95} INFO - [[34m2019-08-27 02:49:59,266[0m] {[34mdiscovery.py:[0m867} INFO[0m - URL being requested: POST https://www.googleapis.com/bigquery/v2/projects/bigdata-staging/jobs?alt=json[0m \r\n    [2019-08-27 02:49:59,266] {taskinstance.py:1047} ERROR - unsupported operand type(s) for +: 'NoneType' and 'int' Traceback (most recent call last):   File \"/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py\", line 922, in _run_raw_task\r\n        result = task_copy.execute(context=context)   \r\n        File \"/airflow/dags/git/subfolder/my_functions_google.py\", line 2502, in execute\r\n        cluster_fields=self.cluster_fields)   \r\n        File \"/airflow/dags/git/subfolder/my_functions_google.py\", line 1396, in run_load\r\n        return self.run_with_configuration(configuration)   \r\n        File \"/airflow/dags/git/subfolder/my_functions_google.py\", line 1414, in run_with_configuration\r\n        .execute(num_retries=self.num_retries)   \r\n        File \"/usr/local/lib/python3.7/site-packages/googleapiclient/_helpers.py\", line 130, in positional_wrapper\r\n        return wrapped(*args, **kwargs)   \r\n        File \"/usr/local/lib/python3.7/site-packages/googleapiclient/http.py\", line 851, in execute\r\n        method=str(self.method), body=self.body, headers=self.headers)   \r\n        File \"/usr/local/lib/python3.7/site-packages/googleapiclient/http.py\", line 153, in _retry_request\r\n        for retry_num in range(num_retries + 1): TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\r\n```\r\n\r\n### Tests\r\n\r\n- [x] My PR adds the following unit tests __OR__ does not need testing for this extremely good reason:\r\n* `TestBigQueryHookWithNumRetries.test_num_retries_is_not_none_by_default`\r\n### Commits\r\n\r\n- [x] My commits all reference Jira issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from \"[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)\":\r\n  1. Subject is separated from body by a blank line\r\n  1. Subject is limited to 50 characters (not including Jira issue reference)\r\n  1. Subject does not end with a period\r\n  1. Subject uses the imperative mood (\"add\", not \"adding\")\r\n  1. Body wraps at 72 characters\r\n  1. Body explains \"what\" and \"why\", not \"how\"\r\n\r\n### Documentation\r\n\r\n- [x] In case of new functionality, my PR adds documentation that describes how to use it.\r\n  - All the public functions and the classes in the PR contain docstrings that explain what it does\r\n  - If you implement backwards incompatible changes, please leave a note in the [Updating.md](https://github.com/apache/airflow/blob/master/UPDATING.md) so we can assign it to a appropriate release\r\n\r\n### Code Quality\r\n\r\n- [x] Passes `flake8`\r\n",
        "date": "2019-08-29",
        "merge_hash": "890adde7bd6d3269400d5e62fbbf99900c2622ac",
        "files": [
            {
                "sha": "c41b35aa3fc46416211b91cec22e426e397dd7d4",
                "filename": "airflow/contrib/hooks/bigquery_hook.py",
                "status": "modified",
                "additions": 3,
                "deletions": 4,
                "changes": 7,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/contrib/hooks/bigquery_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/contrib/hooks/bigquery_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/contrib/hooks/bigquery_hook.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "6963efaa2e123aa76bcc6c934e313e94778543be",
                "filename": "airflow/contrib/hooks/gcp_api_base_hook.py",
                "status": "modified",
                "additions": 10,
                "deletions": 0,
                "changes": 10,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/contrib/hooks/gcp_api_base_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/contrib/hooks/gcp_api_base_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/contrib/hooks/gcp_api_base_hook.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "cf846e1545af7aa22ca4b9b6f00358d81c6af4a9",
                "filename": "airflow/contrib/hooks/gcp_dataproc_hook.py",
                "status": "modified",
                "additions": 1,
                "deletions": 2,
                "changes": 3,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/contrib/hooks/gcp_dataproc_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/contrib/hooks/gcp_dataproc_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/contrib/hooks/gcp_dataproc_hook.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "ca84fc1009feee1bd9edacb8e04d1289b251c092",
                "filename": "airflow/gcp/hooks/cloud_build.py",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/cloud_build.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/cloud_build.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/gcp/hooks/cloud_build.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "dacbf6a771bb74f916f43f063cfbce09b0478233",
                "filename": "airflow/gcp/hooks/cloud_sql.py",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/cloud_sql.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/cloud_sql.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/gcp/hooks/cloud_sql.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "3d4e766c39931c2aee7d42205ca62ccadd8c644b",
                "filename": "airflow/gcp/hooks/cloud_storage_transfer_service.py",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/cloud_storage_transfer_service.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/cloud_storage_transfer_service.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/gcp/hooks/cloud_storage_transfer_service.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "66804220ee4dc71bf4061394a3bb0ca4b6a116ba",
                "filename": "airflow/gcp/hooks/compute.py",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/compute.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/compute.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/gcp/hooks/compute.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "45bd24edac0dd142c7226dbb21a29eeaf292d15e",
                "filename": "airflow/gcp/hooks/dataflow.py",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/dataflow.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/dataflow.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/gcp/hooks/dataflow.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "ca16a6272215564c763e4344918867dedbeb2d32",
                "filename": "airflow/gcp/hooks/datastore.py",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/datastore.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/datastore.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/gcp/hooks/datastore.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "9a26ca5ae1b674de376d76ff3a1022682bacda30",
                "filename": "airflow/gcp/hooks/functions.py",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/functions.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/functions.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/gcp/hooks/functions.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "08b498bba017dd7e2aa55f7cc6f95458fc1699ba",
                "filename": "airflow/gcp/hooks/gsheets.py",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/gsheets.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/gsheets.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/gcp/hooks/gsheets.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "3910ca0148985da76b7763b7a8e8cf5842d11697",
                "filename": "airflow/gcp/hooks/kms.py",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/kms.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/kms.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/gcp/hooks/kms.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "a23aa355e40361ded3cb5afcd5dd07546656aae1",
                "filename": "airflow/gcp/hooks/pubsub.py",
                "status": "modified",
                "additions": 0,
                "deletions": 1,
                "changes": 1,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/pubsub.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/airflow/gcp/hooks/pubsub.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/gcp/hooks/pubsub.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            },
            {
                "sha": "a65ba2148a9b71c7f177454d521a39c3e87330d1",
                "filename": "tests/contrib/hooks/test_gcp_api_base_hook.py",
                "status": "modified",
                "additions": 11,
                "deletions": 0,
                "changes": 11,
                "blob_url": "https://github.com/apache/airflow/blob/890adde7bd6d3269400d5e62fbbf99900c2622ac/tests/contrib/hooks/test_gcp_api_base_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/890adde7bd6d3269400d5e62fbbf99900c2622ac/tests/contrib/hooks/test_gcp_api_base_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/contrib/hooks/test_gcp_api_base_hook.py?ref=890adde7bd6d3269400d5e62fbbf99900c2622ac"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/3831",
        "repo": "airflow",
        "number": 3831,
        "bug_id": "airflow_3831",
        "title": "[AIRFLOW-2981] Fix TypeError in dataflow operators",
        "body": "Make sure you have checked _all_ steps below.\r\n\r\n### Jira\r\n\r\n- [x] My PR addresses the following [Airflow Jira](https://issues.apache.org/jira/browse/AIRFLOW/) issues and references them in the PR title. For example, \"\\[AIRFLOW-XXX\\] My Airflow PR\"\r\n  - https://issues.apache.org/jira/browse/AIRFLOW-2981\r\n  - In case you are fixing a typo in the documentation you can prepend your commit with \\[AIRFLOW-XXX\\], code changes always need a Jira issue.\r\n\r\n### Description\r\n\r\n- [x] Here are some details about my PR, including screenshots of any UI changes:\r\nThe `GoogleCloudBucketHelper.google_cloud_to_local` function attempts to compare a list to an int, resulting in the TypeError, with:\r\n\r\n```\r\n...\r\npath_components = file_name[self.GCS_PREFIX_LENGTH:].split('/')\r\nif path_components < 2:\r\n```\r\n\r\n\r\n### Tests\r\n\r\n- [x] My PR adds the following unit tests __OR__ does not need testing for this extremely good reason:\r\n- `GoogleCloudBucketHelperTest. test_invalid_object_path`\r\n### Commits\r\n\r\n- [x] My commits all reference Jira issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from \"[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)\":\r\n  1. Subject is separated from body by a blank line\r\n  1. Subject is limited to 50 characters (not including Jira issue reference)\r\n  1. Subject does not end with a period\r\n  1. Subject uses the imperative mood (\"add\", not \"adding\")\r\n  1. Body wraps at 72 characters\r\n  1. Body explains \"what\" and \"why\", not \"how\"\r\n\r\n### Documentation\r\n\r\n- [x] In case of new functionality, my PR adds documentation that describes how to use it.\r\n  - When adding new operators/hooks/sensors, the autoclass documentation generation needs to be added.\r\n\r\n### Code Quality\r\n\r\n- [x] Passes `git diff upstream/master -u -- \"*.py\" | flake8 --diff`\r\n",
        "date": "2018-09-01",
        "merge_hash": "db9bb7f57dad69d8cee657f0a901ff64558b3ef9",
        "files": [
            {
                "sha": "3c69fb759abd33eee043a4137e19212efca3a646",
                "filename": "airflow/contrib/operators/dataflow_operator.py",
                "status": "modified",
                "additions": 3,
                "deletions": 3,
                "changes": 6,
                "blob_url": "https://github.com/apache/airflow/blob/db9bb7f57dad69d8cee657f0a901ff64558b3ef9/airflow/contrib/operators/dataflow_operator.py",
                "raw_url": "https://github.com/apache/airflow/raw/db9bb7f57dad69d8cee657f0a901ff64558b3ef9/airflow/contrib/operators/dataflow_operator.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/contrib/operators/dataflow_operator.py?ref=db9bb7f57dad69d8cee657f0a901ff64558b3ef9"
            },
            {
                "sha": "a373126b24e4b6cc8b4a746030fe7c5b2feeefa6",
                "filename": "tests/contrib/operators/test_dataflow_operator.py",
                "status": "modified",
                "additions": 26,
                "deletions": 3,
                "changes": 29,
                "blob_url": "https://github.com/apache/airflow/blob/db9bb7f57dad69d8cee657f0a901ff64558b3ef9/tests/contrib/operators/test_dataflow_operator.py",
                "raw_url": "https://github.com/apache/airflow/raw/db9bb7f57dad69d8cee657f0a901ff64558b3ef9/tests/contrib/operators/test_dataflow_operator.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/contrib/operators/test_dataflow_operator.py?ref=db9bb7f57dad69d8cee657f0a901ff64558b3ef9"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/4674",
        "repo": "airflow",
        "number": 4674,
        "bug_id": "airflow_4674",
        "title": "[AIRFLOW-3742] Fix handling of \"fallback\" for int/boolean config option",
        "body": "Support for a fallback kwarg was added in `AirflowConfigParser.get`\r\nin [AIRFLOW-3742], but, `AirflowConfigParser.getboolean` also needs\r\nto support it.\r\n\r\nMake sure you have checked _all_ steps below.\r\n\r\n### Jira\r\n\r\n- [X] My PR addresses the following [Airflow Jira](https://issues.apache.org/jira/browse/AIRFLOW/) issues and references them in the PR title. For example, \"\\[AIRFLOW-XXX\\] My Airflow PR\"\r\n  - https://issues.apache.org/jira/browse/AIRFLOW-3852\r\n  - In case you are fixing a typo in the documentation you can prepend your commit with \\[AIRFLOW-XXX\\], code changes always need a Jira issue.\r\n\r\n### Description\r\n\r\n- [X] Here are some details about my PR, including screenshots of any UI changes:\r\n\r\nOn the latest master, when I ran `airflow worker`, I got the following error:\r\n```\r\n[2019-02-09 00:12:25,983] {__init__.py:51} INFO - Using executor SequentialExecutor\r\nTraceback (most recent call last):\r\nFile \"/home/tanay/Coding/airflow/.env/bin/airflow\", line 6, in <module>\r\nexec(compile(open(__file__).read(), __file__, 'exec'))\r\nFile \"/home/tanay/Coding/airflow/airflow/bin/airflow\", line 32, in <module>\r\nargs.func(args)\r\nFile \"/home/tanay/Coding/airflow/airflow/utils/cli.py\", line 74, in wrapper\r\nreturn f(*args, **kwargs)\r\nFile \"/home/tanay/Coding/airflow/airflow/bin/cli.py\", line 1058, in worker\r\nif not settings.validate_session():\r\nFile \"/home/tanay/Coding/airflow/airflow/settings.py\", line 226, in validate_session\r\nworker_precheck = conf.getboolean('core', 'worker_precheck', fallback=False)\r\nTypeError: getboolean() got an unexpected keyword argument 'fallback'\r\n```\r\nSupport for a `fallback` kwarg in `AirflowConfigParser.get` was added by @ashb in https://github.com/apache/airflow/pull/4567.\r\nSupport for it also needs to be in `AirflowConfigParser.getboolean`.\r\n\r\n\r\n### Tests\r\n\r\n- [X] My PR adds the following unit tests __OR__ does not need testing for this extremely good reason:\r\nNot Needed.\r\n### Commits\r\n\r\n- [X] My commits all reference Jira issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from \"[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)\":\r\n  1. Subject is separated from body by a blank line\r\n  1. Subject is limited to 50 characters (not including Jira issue reference)\r\n  1. Subject does not end with a period\r\n  1. Subject uses the imperative mood (\"add\", not \"adding\")\r\n  1. Body wraps at 72 characters\r\n  1. Body explains \"what\" and \"why\", not \"how\"\r\n\r\n### Documentation\r\n\r\n- [X] In case of new functionality, my PR adds documentation that describes how to use it.\r\n  - When adding new operators/hooks/sensors, the autoclass documentation generation needs to be added.\r\n  - All the public functions and the classes in the PR contain docstrings that explain what it does\r\n\r\n### Code Quality\r\n\r\n- [X] Passes `flake8`\r\n",
        "date": "2019-02-11",
        "merge_hash": "8917ce6befc8cb69cefc7f11fd069fe2bd94aa8d",
        "files": [
            {
                "sha": "c7fd3bd1417f876b73178876a84791dc8e6fab9e",
                "filename": "airflow/configuration.py",
                "status": "modified",
                "additions": 10,
                "deletions": 10,
                "changes": 20,
                "blob_url": "https://github.com/apache/airflow/blob/8917ce6befc8cb69cefc7f11fd069fe2bd94aa8d/airflow/configuration.py",
                "raw_url": "https://github.com/apache/airflow/raw/8917ce6befc8cb69cefc7f11fd069fe2bd94aa8d/airflow/configuration.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/configuration.py?ref=8917ce6befc8cb69cefc7f11fd069fe2bd94aa8d"
            },
            {
                "sha": "2c6489c04d0328d88dc4346e5510e3ae1dd5345e",
                "filename": "tests/test_configuration.py",
                "status": "modified",
                "additions": 4,
                "deletions": 1,
                "changes": 5,
                "blob_url": "https://github.com/apache/airflow/blob/8917ce6befc8cb69cefc7f11fd069fe2bd94aa8d/tests/test_configuration.py",
                "raw_url": "https://github.com/apache/airflow/raw/8917ce6befc8cb69cefc7f11fd069fe2bd94aa8d/tests/test_configuration.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/test_configuration.py?ref=8917ce6befc8cb69cefc7f11fd069fe2bd94aa8d"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/4384",
        "repo": "airflow",
        "number": 4384,
        "bug_id": "airflow_4384",
        "title": "[AIRFLOW-3578] Fix Type Error for BigQueryOperator",
        "body": "Make sure you have checked _all_ steps below.\r\n\r\n### Jira\r\n\r\n- [x] My PR addresses the following [Airflow Jira](https://issues.apache.org/jira/browse/AIRFLOW/) issues and references them in the PR title. \r\n  - https://issues.apache.org/jira/browse/AIRFLOW-3578\r\n\r\n\r\n### Description\r\n\r\n- [x] Here are some details about my PR, including screenshots of any UI changes:\r\n\r\nThe error is because it just checks for `str` type and not `unicode` but as **`sql` is a templated field it returns unicode and not str**\r\n\r\n**Error**:\r\n```\r\n[2018-12-27 13:33:08,756] {__init__.py:1548} ERROR - query argument must have a type <type 'str'> not <type 'unicode'>\r\nTraceback (most recent call last):\r\n  File \"/Users/kaxil/Documents/GitHub/incubator-airflow/airflow/models/__init__.py\", line 1431, in _run_raw_task\r\n    result = task_copy.execute(context=context)\r\n  File \"/Users/kaxil/Documents/GitHub/incubator-airflow/airflow/contrib/operators/bigquery_operator.py\", line 176, in execute\r\n    cluster_fields=self.cluster_fields,\r\n  File \"/Users/kaxil/Documents/GitHub/incubator-airflow/airflow/contrib/hooks/bigquery_hook.py\", line 677, in run_query\r\n    param_type)\r\n  File \"/Users/kaxil/Documents/GitHub/incubator-airflow/airflow/contrib/hooks/bigquery_hook.py\", line 1903, in _validate_value\r\n    key, expected_type, type(value)))\r\nTypeError: query argument must have a type <type 'str'> not <type 'unicode'>\r\n```\r\n\r\n### Tests\r\n\r\n- [x] My PR adds the following unit tests __OR__ does not need testing for this extremely good reason:\r\nAdded tests\r\n\r\n### Commits\r\n\r\n- [x] My commits all reference Jira issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from \"[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)\":\r\n  1. Subject is separated from body by a blank line\r\n  1. Subject is limited to 50 characters (not including Jira issue reference)\r\n  1. Subject does not end with a period\r\n  1. Subject uses the imperative mood (\"add\", not \"adding\")\r\n  1. Body wraps at 72 characters\r\n  1. Body explains \"what\" and \"why\", not \"how\"\r\n\r\n### Documentation\r\n\r\n- [x] In case of new functionality, my PR adds documentation that describes how to use it.\r\n  - When adding new operators/hooks/sensors, the autoclass documentation generation needs to be added.\r\n  - All the public functions and the classes in the PR contain docstrings that explain what it does\r\n\r\n### Code Quality\r\n\r\n- [x] Passes `flake8`\r\n",
        "date": "2019-01-02",
        "merge_hash": "aa2dc603abce3a4e8885afddd1b267876613a466",
        "files": [
            {
                "sha": "6dabd3ea4a412b2516b2f07c0c8f94ff04dc07ab",
                "filename": "airflow/contrib/hooks/bigquery_hook.py",
                "status": "modified",
                "additions": 3,
                "deletions": 2,
                "changes": 5,
                "blob_url": "https://github.com/apache/airflow/blob/aa2dc603abce3a4e8885afddd1b267876613a466/airflow/contrib/hooks/bigquery_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/aa2dc603abce3a4e8885afddd1b267876613a466/airflow/contrib/hooks/bigquery_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/contrib/hooks/bigquery_hook.py?ref=aa2dc603abce3a4e8885afddd1b267876613a466"
            },
            {
                "sha": "d005fcd519b2d43e994911cce645c255866a0542",
                "filename": "tests/contrib/operators/test_bigquery_operator.py",
                "status": "modified",
                "additions": 33,
                "deletions": 0,
                "changes": 33,
                "blob_url": "https://github.com/apache/airflow/blob/aa2dc603abce3a4e8885afddd1b267876613a466/tests/contrib/operators/test_bigquery_operator.py",
                "raw_url": "https://github.com/apache/airflow/raw/aa2dc603abce3a4e8885afddd1b267876613a466/tests/contrib/operators/test_bigquery_operator.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/contrib/operators/test_bigquery_operator.py?ref=aa2dc603abce3a4e8885afddd1b267876613a466"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/4238",
        "repo": "airflow",
        "number": 4238,
        "bug_id": "airflow_4238",
        "title": "[AIRFLOW-987] pass kerberos cli args keytab and principal to kerberos\u2026",
        "body": "\u2026.run()\r\n\r\nMake sure you have checked _all_ steps below.\r\n\r\n### Jira\r\n\r\n- [x] My PR addresses the following [Airflow Jira](https://issues.apache.org/jira/browse/AIRFLOW/) issues and references them in the PR title. For example, \"\\[AIRFLOW-987]\\] My Airflow PR\"\r\n  - https://issues.apache.org/jira/browse/AIRFLOW-987\r\n  - In case you are fixing a typo in the documentation you can prepend your commit with \\[AIRFLOW-XXX\\], code changes always need a Jira issue.\r\n\r\n### Description\r\n\r\n- [x] Here are some details about my PR, including screenshots of any UI changes:\r\n\r\ndefault=conf.get('kerberos', 'principal')) - removed from cli, because of behavior inside renew_from_kt function. \r\n\r\ninside renew_from_kt we parse conf args one way with one replace type, and if we decide what need to use workaround we parse conf args in perform_krb181_workaround() using the second way\r\n\r\nso, it's not possible to put as default conf value in principal, because this way will be always args.principal and we ca not have 2 different ways how to parse this value\r\n\r\nalso b\"\\n\".join(subp.stdout.readlines()) caused error on 3.5, it was not catched because wasn't covered with tests\r\n\r\nerror in 3.5 - \r\n` b\"\\n\".join(subp.stderr.readlines())))\r\n\r\n   TypeError: sequence item 0: expected a bytes-like object, str found`\r\n### Tests\r\n\r\n- [x] My PR adds the following unit tests __OR__ does not need testing for this extremely good reason:\r\n\r\n### Commits\r\n\r\n- [x] My commits all reference Jira issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from \"[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)\":\r\n  1. Subject is separated from body by a blank line\r\n  1. Subject is limited to 50 characters (not including Jira issue reference)\r\n  1. Subject does not end with a period\r\n  1. Subject uses the imperative mood (\"add\", not \"adding\")\r\n  1. Body wraps at 72 characters\r\n  1. Body explains \"what\" and \"why\", not \"how\"\r\n\r\n### Documentation\r\n\r\n- [x] In case of new functionality, my PR adds documentation that describes how to use it.\r\n  - When adding new operators/hooks/sensors, the autoclass documentation generation needs to be added.\r\n\r\n### Code Quality\r\n\r\n- [x] Passes `flake8`\r\n",
        "date": "2018-12-09",
        "merge_hash": "e947c6c034238ede29a6c8f51307458d3e40c1b5",
        "files": [
            {
                "sha": "f2342ee1486f9f8b422c14e5b1742b7b48fa4817",
                "filename": "airflow/bin/cli.py",
                "status": "modified",
                "additions": 3,
                "deletions": 4,
                "changes": 7,
                "blob_url": "https://github.com/apache/airflow/blob/e947c6c034238ede29a6c8f51307458d3e40c1b5/airflow/bin/cli.py",
                "raw_url": "https://github.com/apache/airflow/raw/e947c6c034238ede29a6c8f51307458d3e40c1b5/airflow/bin/cli.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/bin/cli.py?ref=e947c6c034238ede29a6c8f51307458d3e40c1b5"
            },
            {
                "sha": "b91241bc1de1c46a50454ca56998d4a7530723fc",
                "filename": "airflow/security/kerberos.py",
                "status": "modified",
                "additions": 15,
                "deletions": 15,
                "changes": 30,
                "blob_url": "https://github.com/apache/airflow/blob/e947c6c034238ede29a6c8f51307458d3e40c1b5/airflow/security/kerberos.py",
                "raw_url": "https://github.com/apache/airflow/raw/e947c6c034238ede29a6c8f51307458d3e40c1b5/airflow/security/kerberos.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/security/kerberos.py?ref=e947c6c034238ede29a6c8f51307458d3e40c1b5"
            },
            {
                "sha": "d3d7bd5b0085790e2f8446cf43156d3ae6a2e094",
                "filename": "tests/security/test_kerberos.py",
                "status": "modified",
                "additions": 31,
                "deletions": 6,
                "changes": 37,
                "blob_url": "https://github.com/apache/airflow/blob/e947c6c034238ede29a6c8f51307458d3e40c1b5/tests/security/test_kerberos.py",
                "raw_url": "https://github.com/apache/airflow/raw/e947c6c034238ede29a6c8f51307458d3e40c1b5/tests/security/test_kerberos.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/security/test_kerberos.py?ref=e947c6c034238ede29a6c8f51307458d3e40c1b5"
            }
        ]
    },
    {
        "pr": "https://github.com/apache/airflow/pull/3690",
        "repo": "airflow",
        "number": 3690,
        "bug_id": "airflow_3690",
        "title": "[AIRFLOW-2845] Remove asserts from the contrib package",
        "body": "Make sure you have checked _all_ steps below.\r\n\r\n### Jira\r\n\r\n- [x] My PR addresses the following [AIRFLOW-2845](https://issues.apache.org/jira/projects/AIRFLOW/issues/AIRFLOW-2845) issues and references them in the PR title. For example, \"\\[AIRFLOW-XXX\\] My Airflow PR\"\r\n  - https://issues.apache.org/jira/browse/AIRFLOW-XXX\r\n  - In case you are fixing a typo in the documentation you can prepend your commit with \\[AIRFLOW-XXX\\], code changes always need a Jira issue.\r\n\r\n### Description\r\n\r\n- [x] Here are some details about my PR, including screenshots of any UI changes:\r\n`asserts` is used in Airflow contrib package code .  And from point of view for which purposes asserts are really is, it's not correct.\r\n\r\nIf we look at documentation we could find information what asserts is debug tool: https://docs.python.org/3/reference/simple_stmts.html#the-assert-statement and also it is could be disabled globally by default. \r\n\r\nSo, I just want to change debug asserts to ValueError and TypeError.\r\n### Tests\r\n\r\n- [x] My PR adds the following unit tests __OR__ does not need testing for this extremely good reason:\r\nIt's covered by existing tests. No new features or important changes. \r\n\r\n### Commits\r\n\r\n- [x] My commits all reference Jira issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from \"[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)\":\r\n  1. Subject is separated from body by a blank line\r\n  1. Subject is limited to 50 characters (not including Jira issue reference)\r\n  1. Subject does not end with a period\r\n  1. Subject uses the imperative mood (\"add\", not \"adding\")\r\n  1. Body wraps at 72 characters\r\n  1. Body explains \"what\" and \"why\", not \"how\"\r\n\r\n### Documentation\r\n\r\n- [x] In case of new functionality, my PR adds documentation that describes how to use it.\r\n  - When adding new operators/hooks/sensors, the autoclass documentation generation needs to be added.\r\n\r\n### Code Quality\r\n\r\n- [x] Passes `git diff upstream/master -u -- \"*.py\" | flake8 --diff`\r\n",
        "date": "2018-08-06",
        "merge_hash": "5b7a28c7e0dade62bb4d087f25687821e7f11083",
        "files": [
            {
                "sha": "aa8fc382a6a67c0a2d889e01a010f3d13ef37588",
                "filename": "airflow/contrib/hooks/bigquery_hook.py",
                "status": "modified",
                "additions": 29,
                "deletions": 26,
                "changes": 55,
                "blob_url": "https://github.com/apache/airflow/blob/5b7a28c7e0dade62bb4d087f25687821e7f11083/airflow/contrib/hooks/bigquery_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/5b7a28c7e0dade62bb4d087f25687821e7f11083/airflow/contrib/hooks/bigquery_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/contrib/hooks/bigquery_hook.py?ref=5b7a28c7e0dade62bb4d087f25687821e7f11083"
            },
            {
                "sha": "2e5f1399b4765c8b3e3cc8ed3b2f44d6126f54a3",
                "filename": "airflow/contrib/hooks/databricks_hook.py",
                "status": "modified",
                "additions": 2,
                "deletions": 1,
                "changes": 3,
                "blob_url": "https://github.com/apache/airflow/blob/5b7a28c7e0dade62bb4d087f25687821e7f11083/airflow/contrib/hooks/databricks_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/5b7a28c7e0dade62bb4d087f25687821e7f11083/airflow/contrib/hooks/databricks_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/contrib/hooks/databricks_hook.py?ref=5b7a28c7e0dade62bb4d087f25687821e7f11083"
            },
            {
                "sha": "ee3b510ed706aa9cde4c69a1f95df2269ab158ca",
                "filename": "airflow/contrib/hooks/gcp_dataflow_hook.py",
                "status": "modified",
                "additions": 14,
                "deletions": 10,
                "changes": 24,
                "blob_url": "https://github.com/apache/airflow/blob/5b7a28c7e0dade62bb4d087f25687821e7f11083/airflow/contrib/hooks/gcp_dataflow_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/5b7a28c7e0dade62bb4d087f25687821e7f11083/airflow/contrib/hooks/gcp_dataflow_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/contrib/hooks/gcp_dataflow_hook.py?ref=5b7a28c7e0dade62bb4d087f25687821e7f11083"
            },
            {
                "sha": "b9f1008fa7fde12a7d315b4b9910f8188ee34ea7",
                "filename": "airflow/contrib/hooks/gcp_mlengine_hook.py",
                "status": "modified",
                "additions": 8,
                "deletions": 3,
                "changes": 11,
                "blob_url": "https://github.com/apache/airflow/blob/5b7a28c7e0dade62bb4d087f25687821e7f11083/airflow/contrib/hooks/gcp_mlengine_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/5b7a28c7e0dade62bb4d087f25687821e7f11083/airflow/contrib/hooks/gcp_mlengine_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/contrib/hooks/gcp_mlengine_hook.py?ref=5b7a28c7e0dade62bb4d087f25687821e7f11083"
            },
            {
                "sha": "08d44ce7faf92a9125cad737468cae31ce245ea8",
                "filename": "airflow/contrib/hooks/gcs_hook.py",
                "status": "modified",
                "additions": 8,
                "deletions": 7,
                "changes": 15,
                "blob_url": "https://github.com/apache/airflow/blob/5b7a28c7e0dade62bb4d087f25687821e7f11083/airflow/contrib/hooks/gcs_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/5b7a28c7e0dade62bb4d087f25687821e7f11083/airflow/contrib/hooks/gcs_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/contrib/hooks/gcs_hook.py?ref=5b7a28c7e0dade62bb4d087f25687821e7f11083"
            },
            {
                "sha": "8e75b3c608238c0ee78816518edf944f25665a4b",
                "filename": "airflow/contrib/operators/mlengine_operator.py",
                "status": "modified",
                "additions": 3,
                "deletions": 1,
                "changes": 4,
                "blob_url": "https://github.com/apache/airflow/blob/5b7a28c7e0dade62bb4d087f25687821e7f11083/airflow/contrib/operators/mlengine_operator.py",
                "raw_url": "https://github.com/apache/airflow/raw/5b7a28c7e0dade62bb4d087f25687821e7f11083/airflow/contrib/operators/mlengine_operator.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/airflow/contrib/operators/mlengine_operator.py?ref=5b7a28c7e0dade62bb4d087f25687821e7f11083"
            },
            {
                "sha": "39435f0c4ec636a45b4b09ae37d8932db23dcac4",
                "filename": "tests/contrib/hooks/test_bigquery_hook.py",
                "status": "modified",
                "additions": 1,
                "deletions": 1,
                "changes": 2,
                "blob_url": "https://github.com/apache/airflow/blob/5b7a28c7e0dade62bb4d087f25687821e7f11083/tests/contrib/hooks/test_bigquery_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/5b7a28c7e0dade62bb4d087f25687821e7f11083/tests/contrib/hooks/test_bigquery_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/contrib/hooks/test_bigquery_hook.py?ref=5b7a28c7e0dade62bb4d087f25687821e7f11083"
            },
            {
                "sha": "aca8dd96004b400662028149c4c61e76c8908ae1",
                "filename": "tests/contrib/hooks/test_databricks_hook.py",
                "status": "modified",
                "additions": 3,
                "deletions": 3,
                "changes": 6,
                "blob_url": "https://github.com/apache/airflow/blob/5b7a28c7e0dade62bb4d087f25687821e7f11083/tests/contrib/hooks/test_databricks_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/5b7a28c7e0dade62bb4d087f25687821e7f11083/tests/contrib/hooks/test_databricks_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/contrib/hooks/test_databricks_hook.py?ref=5b7a28c7e0dade62bb4d087f25687821e7f11083"
            },
            {
                "sha": "bc7c587135b9ba49324cdbbcfec378d2d924fca3",
                "filename": "tests/contrib/hooks/test_gcp_dataflow_hook.py",
                "status": "modified",
                "additions": 4,
                "deletions": 4,
                "changes": 8,
                "blob_url": "https://github.com/apache/airflow/blob/5b7a28c7e0dade62bb4d087f25687821e7f11083/tests/contrib/hooks/test_gcp_dataflow_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/5b7a28c7e0dade62bb4d087f25687821e7f11083/tests/contrib/hooks/test_gcp_dataflow_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/contrib/hooks/test_gcp_dataflow_hook.py?ref=5b7a28c7e0dade62bb4d087f25687821e7f11083"
            },
            {
                "sha": "eedceff1f779032c3b2462b75565f8f36898d5dd",
                "filename": "tests/contrib/hooks/test_gcs_hook.py",
                "status": "modified",
                "additions": 2,
                "deletions": 2,
                "changes": 4,
                "blob_url": "https://github.com/apache/airflow/blob/5b7a28c7e0dade62bb4d087f25687821e7f11083/tests/contrib/hooks/test_gcs_hook.py",
                "raw_url": "https://github.com/apache/airflow/raw/5b7a28c7e0dade62bb4d087f25687821e7f11083/tests/contrib/hooks/test_gcs_hook.py",
                "contents_url": "https://api.github.com/repos/apache/airflow/contents/tests/contrib/hooks/test_gcs_hook.py?ref=5b7a28c7e0dade62bb4d087f25687821e7f11083"
            }
        ]
    }
]