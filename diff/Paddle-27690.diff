diff --git a/python/paddle/fluid/tests/unittests/test_multiply.py b/python/paddle/fluid/tests/unittests/test_multiply.py
index dbf167617a24..abd0c15dc723 100755
--- a/python/paddle/fluid/tests/unittests/test_multiply.py
+++ b/python/paddle/fluid/tests/unittests/test_multiply.py
@@ -26,6 +26,7 @@ class TestMultiplyAPI(unittest.TestCase):
 
     def __run_static_graph_case(self, x_data, y_data, axis=-1):
         with program_guard(Program(), Program()):
+            paddle.enable_static()
             x = paddle.static.data(
                 name='x', shape=x_data.shape, dtype=x_data.dtype)
             y = paddle.static.data(
@@ -42,6 +43,21 @@ def __run_static_graph_case(self, x_data, y_data, axis=-1):
             res = outs[0]
             return res
 
+    def __run_static_graph_case_with_numpy_input(self, x_data, y_data, axis=-1):
+        with program_guard(Program(), Program()):
+            paddle.enable_static()
+
+            res = tensor.multiply(x_data, y_data, axis=axis)
+            place = fluid.CUDAPlace(0) if fluid.core.is_compiled_with_cuda(
+            ) else fluid.CPUPlace()
+            exe = fluid.Executor(place)
+            outs = exe.run(fluid.default_main_program(),
+                           feed={'x': x_data,
+                                 'y': y_data},
+                           fetch_list=[res])
+            res = outs[0]
+            return res
+
     def __run_dynamic_graph_case(self, x_data, y_data, axis=-1):
         paddle.disable_static()
         x = paddle.to_tensor(x_data)
@@ -49,27 +65,52 @@ def __run_dynamic_graph_case(self, x_data, y_data, axis=-1):
         res = paddle.multiply(x, y, axis=axis)
         return res.numpy()
 
+    def __run_dynamic_graph_case_with_numpy_input(self, x_data, y_data,
+                                                  axis=-1):
+        paddle.disable_static()
+        res = paddle.multiply(x_data, y_data, axis=axis)
+        return res.numpy()
+
     def test_multiply(self):
         """test_multiply."""
         np.random.seed(7)
+
         # test static computation graph: 1-d array
         x_data = np.random.rand(200)
         y_data = np.random.rand(200)
         res = self.__run_static_graph_case(x_data, y_data)
         self.assertTrue(np.allclose(res, np.multiply(x_data, y_data)))
 
+        # test static computation graph: 1-d array
+        x_data = np.random.rand(200)
+        y_data = np.random.rand(200)
+        res = self.__run_static_graph_case_with_numpy_input(x_data, y_data)
+        self.assertTrue(np.allclose(res, np.multiply(x_data, y_data)))
+
         # test static computation graph: 2-d array
         x_data = np.random.rand(2, 500)
         y_data = np.random.rand(2, 500)
         res = self.__run_static_graph_case(x_data, y_data)
         self.assertTrue(np.allclose(res, np.multiply(x_data, y_data)))
 
+        # test static computation graph with_primitives: 2-d array
+        x_data = np.random.rand(2, 500)
+        y_data = np.random.rand(2, 500)
+        res = self.__run_static_graph_case_with_numpy_input(x_data, y_data)
+        self.assertTrue(np.allclose(res, np.multiply(x_data, y_data)))
+
         # test static computation graph: broadcast
         x_data = np.random.rand(2, 500)
         y_data = np.random.rand(500)
         res = self.__run_static_graph_case(x_data, y_data)
         self.assertTrue(np.allclose(res, np.multiply(x_data, y_data)))
 
+        # test static computation graph with_primitives: broadcast
+        x_data = np.random.rand(2, 500)
+        y_data = np.random.rand(500)
+        res = self.__run_static_graph_case_with_numpy_input(x_data, y_data)
+        self.assertTrue(np.allclose(res, np.multiply(x_data, y_data)))
+
         # test static computation graph: broadcast with axis
         x_data = np.random.rand(2, 300, 40)
         y_data = np.random.rand(300)
@@ -77,24 +118,50 @@ def test_multiply(self):
         expected = np.multiply(x_data, y_data[..., np.newaxis])
         self.assertTrue(np.allclose(res, expected))
 
+        # test static computation graph with_primitives: broadcast with axis
+        x_data = np.random.rand(2, 300, 40)
+        y_data = np.random.rand(300)
+        res = self.__run_static_graph_case_with_numpy_input(
+            x_data, y_data, axis=1)
+        expected = np.multiply(x_data, y_data[..., np.newaxis])
+        self.assertTrue(np.allclose(res, expected))
+
         # test dynamic computation graph: 1-d array
         x_data = np.random.rand(200)
         y_data = np.random.rand(200)
         res = self.__run_dynamic_graph_case(x_data, y_data)
         self.assertTrue(np.allclose(res, np.multiply(x_data, y_data)))
 
+        # test dynamic numpy input computation graph: 1-d array
+        x_data = np.random.rand(200)
+        y_data = np.random.rand(200)
+        res = self.__run_dynamic_graph_case_with_numpy_input(x_data, y_data)
+        self.assertTrue(np.allclose(res, np.multiply(x_data, y_data)))
+
         # test dynamic computation graph: 2-d array
         x_data = np.random.rand(20, 50)
         y_data = np.random.rand(20, 50)
         res = self.__run_dynamic_graph_case(x_data, y_data)
         self.assertTrue(np.allclose(res, np.multiply(x_data, y_data)))
 
+        # test dynamic numpy input computation graph: 1-d array
+        x_data = np.random.rand(20, 50)
+        y_data = np.random.rand(20, 50)
+        res = self.__run_dynamic_graph_case_with_numpy_input(x_data, y_data)
+        self.assertTrue(np.allclose(res, np.multiply(x_data, y_data)))
+
         # test dynamic computation graph: broadcast
         x_data = np.random.rand(2, 500)
         y_data = np.random.rand(500)
         res = self.__run_dynamic_graph_case(x_data, y_data)
         self.assertTrue(np.allclose(res, np.multiply(x_data, y_data)))
 
+        # test dynamic computation graph with numpy tensor: broadcast
+        x_data = np.random.rand(2, 500)
+        y_data = np.random.rand(500)
+        res = self.__run_dynamic_graph_case_with_numpy_input(x_data, y_data)
+        self.assertTrue(np.allclose(res, np.multiply(x_data, y_data)))
+
         # test dynamic computation graph: broadcast with axis
         x_data = np.random.rand(2, 300, 40)
         y_data = np.random.rand(300)
@@ -102,6 +169,14 @@ def test_multiply(self):
         expected = np.multiply(x_data, y_data[..., np.newaxis])
         self.assertTrue(np.allclose(res, expected))
 
+        # test dynamic computation graph with numpy tensor: broadcast with axis
+        x_data = np.random.rand(2, 300, 40)
+        y_data = np.random.rand(300)
+        res = self.__run_dynamic_graph_case_with_numpy_input(
+            x_data, y_data, axis=1)
+        expected = np.multiply(x_data, y_data[..., np.newaxis])
+        self.assertTrue(np.allclose(res, expected))
+
 
 class TestMultiplyError(unittest.TestCase):
     """TestMultiplyError."""
diff --git a/python/paddle/tensor/math.py b/python/paddle/tensor/math.py
index ce32fb76f5cd..09991b4dbe45 100755
--- a/python/paddle/tensor/math.py
+++ b/python/paddle/tensor/math.py
@@ -472,15 +472,27 @@ def multiply(x, y, axis=-1, name=None):
     """
     op_type = 'elementwise_mul'
     act = None
+
     if x.dtype != y.dtype:
         raise TypeError(
             'Input tensors must be same type, but received type of x: %s, type of y: %s '
             % (x.dtype, y.dtype))
 
     if in_dygraph_mode():
+        if not isinstance(x, (paddle.Tensor)):
+            x = paddle.to_tensor(x)
+        if not isinstance(y, (paddle.Tensor)):
+            y = paddle.to_tensor(y)
         return _elementwise_op_in_dygraph(
             x, y, axis=axis, act=act, op_name=op_type)
 
+    if not isinstance(x, (paddle.Tensor, Variable)):
+        x = paddle.static.data(
+            name='x', shape=x.shape, dtype=x.dtype)
+    if not isinstance(y, (paddle.Tensor, Variable)):
+        y = paddle.static.data(
+            name='y', shape=y.shape, dtype=y.dtype)
+
     return _elementwise_op(LayerHelper(op_type, **locals()))
 
 def maximum(x, y, axis=-1, name=None):
