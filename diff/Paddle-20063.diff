diff --git a/paddle/fluid/operators/reduce_ops/reduce_op.h b/paddle/fluid/operators/reduce_ops/reduce_op.h
index 01fb46fe9d7c..cbc4adf95881 100644
--- a/paddle/fluid/operators/reduce_ops/reduce_op.h
+++ b/paddle/fluid/operators/reduce_ops/reduce_op.h
@@ -165,13 +165,20 @@ class ReduceOp : public framework::OperatorWithKernel {
                    "Output(Out) of ReduceOp should not be null.");
     auto x_dims = ctx->GetInputDim("X");
     auto x_rank = x_dims.size();
-    PADDLE_ENFORCE_LE(x_rank, 6, "Tensors with rank at most 6 are supported.");
+    PADDLE_ENFORCE_LE(x_rank, 6,
+                      "ShapeError: The input tensor X's dimensions of Reduce "
+                      "should be less equal than 6. But received X's "
+                      "dimensions = %d, X's shape = [%s].",
+                      x_rank, x_dims);
     auto dims = ctx->Attrs().Get<std::vector<int>>("dim");
+
     for (size_t i = 0; i < dims.size(); ++i) {
+      PADDLE_ENFORCE_LT(dims[i], x_rank,
+                        "ShapeError: The reduce dim index %d should be in the "
+                        "range [-dimension(X), dimension(X)]."
+                        "which dimesion = %d, But received dim index = %d",
+                        i, x_rank, dims[i]);
       if (dims[i] < 0) dims[i] = x_rank + dims[i];
-      PADDLE_ENFORCE_LT(
-          dims[i], x_rank,
-          "The dim should be in the range [-rank(input), rank(input)).");
     }
     sort(dims.begin(), dims.end());
     bool reduce_all = ctx->Attrs().Get<bool>("reduce_all");
@@ -202,7 +209,7 @@ class ReduceOp : public framework::OperatorWithKernel {
       }
       auto out_dims = framework::make_ddim(dims_vector);
       ctx->SetOutputDim("Out", out_dims);
-      if (dims[0] != 0) {
+      if (dims.size() > 0 && dims[0] != 0) {
         // Only pass LoD when not reducing on the first dim.
         ctx->ShareLoD("X", /*->*/ "Out");
       }
@@ -223,10 +230,12 @@ class ReduceGradOp : public framework::OperatorWithKernel {
     PADDLE_ENFORCE_LE(x_rank, 6, "Tensors with rank at most 6 are supported.");
     auto dims = ctx->Attrs().Get<std::vector<int>>("dim");
     for (size_t i = 0; i < dims.size(); ++i) {
+      PADDLE_ENFORCE_LT(dims[i], x_rank,
+                        "ShapeError: The reduce dim index %d should be in the "
+                        "range [-dimension(X), dimension(X)]."
+                        "which dimesion = %d, But received dim index = %d",
+                        i, x_rank, dims[i]);
       if (dims[i] < 0) dims[i] = x_rank + dims[i];
-      PADDLE_ENFORCE_LT(
-          dims[i], x_rank,
-          "The dim should be in the range [-rank(input), rank(input)).");
     }
     sort(dims.begin(), dims.end());
     auto x_grad_name = framework::GradVarName("X");
diff --git a/python/paddle/fluid/layers/nn.py b/python/paddle/fluid/layers/nn.py
index 8f6c8a5d1279..5847c7cb8535 100755
--- a/python/paddle/fluid/layers/nn.py
+++ b/python/paddle/fluid/layers/nn.py
@@ -5074,6 +5074,15 @@ def reduce_sum(input, dim=None, keep_dim=False, name=None):
 
     """
     helper = LayerHelper('reduce_sum', **locals())
+    if not isinstance(input, Variable):
+        raise TypeError(
+            "The type of 'input' in reduce_sum must be Variable, but received %s"
+            % (type(input)))
+    if convert_dtype(
+            input.dtype) not in ['float32', 'float64', 'int32', 'int64']:
+        raise TypeError(
+            "The data type of 'input' in reduce_sum  must be float32 or float64 or int32 or int64, but received %s."
+            % (convert_dtype(input.dtype)))
     out = helper.create_variable_for_type_inference(dtype=helper.input_dtype())
     if dim is not None and not isinstance(dim, list):
         dim = [dim]
@@ -5133,6 +5142,15 @@ def reduce_mean(input, dim=None, keep_dim=False, name=None):
             fluid.layers.reduce_mean(y, dim=[0, 1]) # [4.0, 5.0]
     """
     helper = LayerHelper('reduce_mean', **locals())
+    if not isinstance(input, Variable):
+        raise TypeError(
+            "The type of 'input' in reduce_mean must be Variable, but received %s"
+            % (type(input)))
+    if convert_dtype(
+            input.dtype) not in ['float32', 'float64', 'int32', 'int64']:
+        raise TypeError(
+            "The data type of 'input' in reduce_mean  must be float32 or float64 or int32 or int64, but received %s."
+            % (convert_dtype(input.dtype)))
     out = helper.create_variable_for_type_inference(dtype=helper.input_dtype())
     if dim is not None and not isinstance(dim, list):
         dim = [dim]
diff --git a/python/paddle/fluid/tests/test_if_else_op.py b/python/paddle/fluid/tests/test_if_else_op.py
index 61d81f483636..0cadb747835e 100644
--- a/python/paddle/fluid/tests/test_if_else_op.py
+++ b/python/paddle/fluid/tests/test_if_else_op.py
@@ -183,7 +183,7 @@ def compare_ifelse_op_and_numpy(self, place):
                 false_target = fluid.layers.tanh(false_target)
                 ie.output(false_target)
             if_out = ie()
-            out = layers.reduce_sum(if_out)
+            out = layers.reduce_sum(if_out[0])
 
             exe = fluid.Executor(place)
             exe.run(fluid.default_startup_program())
diff --git a/python/paddle/fluid/tests/unittests/test_reduce_op.py b/python/paddle/fluid/tests/unittests/test_reduce_op.py
index a690644d0528..866fa05fdb04 100644
--- a/python/paddle/fluid/tests/unittests/test_reduce_op.py
+++ b/python/paddle/fluid/tests/unittests/test_reduce_op.py
@@ -17,6 +17,9 @@
 import unittest
 import numpy as np
 from op_test import OpTest
+import paddle.fluid.core as core
+import paddle.fluid as fluid
+from paddle.fluid import compiler, Program, program_guard
 
 
 class TestSumOp(OpTest):
@@ -411,5 +414,29 @@ def test_check_grad(self):
         self.check_grad(['X'], 'Out')
 
 
+class TestReduceSumOpError(OpTest):
+    def test_errors(self):
+        with program_guard(Program(), Program()):
+            # The input type of reduce_sum_op must be Variable.
+            x1 = fluid.create_lod_tensor(
+                np.array([[-1]]), [[1]], fluid.CPUPlace())
+            self.assertRaises(TypeError, fluid.layers.reduce_sum, x1)
+            # The input dtype of reduce_sum_op  must be float32 or float64 or int32 or int64.
+            x2 = fluid.layers.data(name='x2', shape=[4], dtype="uint8")
+            self.assertRaises(TypeError, fluid.layers.reduce_sum, x2)
+
+
+class TestReduceMeanOpError(OpTest):
+    def test_errors(self):
+        with program_guard(Program(), Program()):
+            # The input type of reduce_mean_op must be Variable.
+            x1 = fluid.create_lod_tensor(
+                np.array([[-1]]), [[1]], fluid.CPUPlace())
+            self.assertRaises(TypeError, fluid.layers.reduce_mean, x1)
+            # The input dtype of reduce_mean_op  must be float32 or float64 or int32 or int64.
+            x2 = fluid.layers.data(name='x2', shape=[4], dtype="uint8")
+            self.assertRaises(TypeError, fluid.layers.reduce_mean, x2)
+
+
 if __name__ == '__main__':
     unittest.main()
